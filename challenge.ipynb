{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaLazar07/VeridionChallenge/blob/main/challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57654eb4",
      "metadata": {
        "id": "57654eb4"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77307dbd",
      "metadata": {
        "id": "77307dbd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a52118",
      "metadata": {
        "id": "39a52118"
      },
      "source": [
        "**Reading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5192572",
      "metadata": {
        "id": "a5192572"
      },
      "outputs": [],
      "source": [
        "insurance_df = pd.read_csv('ml_insurance_challenge.csv')\n",
        "label_df = pd.read_csv('insurance_taxonomy.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ce8f1a",
      "metadata": {
        "id": "d7ce8f1a"
      },
      "source": [
        "**Info about the Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_df"
      ],
      "metadata": {
        "id": "Av5CtZZKNe-n"
      },
      "id": "Av5CtZZKNe-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df"
      ],
      "metadata": {
        "id": "6uqrqKMZNgtU"
      },
      "id": "6uqrqKMZNgtU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbe2149",
      "metadata": {
        "id": "adbe2149"
      },
      "outputs": [],
      "source": [
        "insurance_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cb29d6",
      "metadata": {
        "id": "75cb29d6"
      },
      "outputs": [],
      "source": [
        "insurance_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f0a303",
      "metadata": {
        "id": "b7f0a303"
      },
      "source": [
        "**Text Preprocessing prior to Company Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25861158",
      "metadata": {
        "id": "25861158"
      },
      "outputs": [],
      "source": [
        "def create_text_about_company(row):\n",
        "    if (pd.notna(row['business_tags'])):\n",
        "        business_tags_processed = row['business_tags'].replace(\"['\", \"\").replace(\"']\", \"\").replace(\"'\", \"\")\n",
        "\n",
        "    description = row['description'] if pd.notna(row['description']) else \"\"\n",
        "\n",
        "    business_tags = business_tags_processed if pd.notna(business_tags_processed) else \"\"\n",
        "\n",
        "    sector = row['sector'] if pd.notna(row['sector']) else \"\"\n",
        "\n",
        "    category = row['category'] if pd.notna(row['category']) else \"\"\n",
        "\n",
        "    niche = row['niche'] if pd.notna(row['niche']) else \"\"\n",
        "\n",
        "    text = f\"{description} Services: {business_tags}. Industry: {sector} - {category} - {niche}\"\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e459d0f",
      "metadata": {
        "id": "5e459d0f"
      },
      "outputs": [],
      "source": [
        "# testing to see what model to use based on the number of tokens accepted by it\n",
        "\n",
        "rows = insurance_df.shape[0]\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "for i in range(rows):\n",
        "    string_row = create_text_about_company(insurance_df.iloc[i])\n",
        "    tokens = tokenizer.encode(str(string_row))\n",
        "    if (len(tokens)) > 512:\n",
        "        print(\"A model with a bigger number of tokens is needed\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d09889",
      "metadata": {
        "id": "28d09889"
      },
      "source": [
        "**Model Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f3da30",
      "metadata": {
        "id": "c6f3da30"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
        "\n",
        "company_text = []\n",
        "\n",
        "for i in range(rows):\n",
        "    string_row = create_text_about_company(insurance_df.iloc[i])\n",
        "    company_text.append(string_row)\n",
        "\n",
        "company_embeddings = model.encode(company_text)\n",
        "label_embeddings = model.encode(label_df['label'].tolist())\n",
        "\n",
        "similarity = util.cos_sim(company_embeddings, label_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e640a5f",
      "metadata": {
        "id": "9e640a5f"
      },
      "source": [
        "**Veryfing most similar labels for each company**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68bead8b",
      "metadata": {
        "id": "68bead8b"
      },
      "source": [
        "**Bi-Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4483553c",
      "metadata": {
        "id": "4483553c"
      },
      "outputs": [],
      "source": [
        "k = 5\n",
        "for i in range(rows):\n",
        "    company_similarity = similarity[i]\n",
        "\n",
        "    top_k_labels = company_similarity.topk(k=k)\n",
        "\n",
        "    label_index = top_k_labels.indices[0].item()\n",
        "    label_name = label_df.iloc[label_index]['label']\n",
        "\n",
        "    insurance_df.loc[i, \"label_insurance\"] = label_name\n",
        "\n",
        "insurance_df.to_csv('ml_insurance_challenge_with_bi_encoder.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d30c6bd0",
      "metadata": {
        "id": "d30c6bd0"
      },
      "source": [
        "**Cross-Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29510be3",
      "metadata": {
        "id": "29510be3"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
        "\n",
        "for i in range(rows):\n",
        "    query = company_text[i]\n",
        "    corpus = label_df['label'].tolist()\n",
        "\n",
        "    ranks = model.rank(query, corpus)\n",
        "    label_name = corpus[ranks[0]['corpus_id']]\n",
        "\n",
        "    insurance_df.loc[i, \"label_insurance\"] = label_name\n",
        "\n",
        "insurance_df.to_csv(\"ml_insurance_challenge_with_cross_encoder.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd0578e",
      "metadata": {
        "id": "abd0578e"
      },
      "source": [
        "**Bi-Encoder + Cross-Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43764631",
      "metadata": {
        "id": "43764631"
      },
      "outputs": [],
      "source": [
        "k = 10\n",
        "best_k = 5\n",
        "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "for i in range(rows):\n",
        "    company_similarity = similarity[i]\n",
        "    query = company_text[i]\n",
        "\n",
        "    top_k_labels = company_similarity.topk(k=k)\n",
        "    best_labels = []\n",
        "\n",
        "    for index in range(best_k):\n",
        "        label_index = top_k_labels.indices[index].item()\n",
        "        best_labels.append(label_df.iloc[label_index]['label'])\n",
        "\n",
        "    corpus = best_labels\n",
        "\n",
        "    ranks = model.rank(query, corpus)\n",
        "    label_name = corpus[ranks[0]['corpus_id']]\n",
        "\n",
        "    insurance_df.loc[i, \"label_insurance\"] = label_name\n",
        "\n",
        "insurance_df.to_csv(\"ml_insurance_challenge_with_bi_and_cross_encoder.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qM2NBoGkKAVh",
      "metadata": {
        "id": "qM2NBoGkKAVh"
      },
      "source": [
        "**Results overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xonFKfwgfox3",
      "metadata": {
        "id": "xonFKfwgfox3"
      },
      "source": [
        "After analyzing the results from using a bi-encoder, cross-encoder and bi + cross encoder model, I noticed that the best results were obtained by using the bi and the bi + cross encoder. Although, I was not satisfied with the results so I decided to attack the challenge with a new approach. I will change the weight for all the features except the description and I also changed the pretrained models used for embedding and cross-encoder to a DeBERTa-v3 based model who has a much higher performance in these types of situations than the ones used before."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wEPlWc1-pP9i",
      "metadata": {
        "id": "wEPlWc1-pP9i"
      },
      "source": [
        "**Weight change for features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2_MU-D9k2E9",
      "metadata": {
        "id": "a2_MU-D9k2E9"
      },
      "outputs": [],
      "source": [
        "def create_text_about_company(row):\n",
        "    business_tags_processed = \"\"\n",
        "    if (pd.notna(row['business_tags'])):\n",
        "        business_tags_processed = row['business_tags'].replace(\"['\", \"\").replace(\"']\", \"\").replace(\"'\", \"\")\n",
        "\n",
        "    description = row['description'] if pd.notna(row['description']) else \"\"\n",
        "\n",
        "    business_tags = business_tags_processed if pd.notna(business_tags_processed) else \"\"\n",
        "\n",
        "    sector = row['sector'] if pd.notna(row['sector']) else \"\"\n",
        "\n",
        "    category = row['category'] if pd.notna(row['category']) else \"\"\n",
        "\n",
        "    niche = row['niche'] if pd.notna(row['niche']) else \"\"\n",
        "\n",
        "    text = f\"{description} Services: {business_tags}. Industry: {sector} - {category} - {niche}\"\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46Gag_F6wUPd",
      "metadata": {
        "id": "46Gag_F6wUPd"
      },
      "source": [
        "**Using DeBERTa-V3 model for embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6V7fnFX_mGbE",
      "metadata": {
        "id": "6V7fnFX_mGbE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "def deberta_embeddings(text, tokenizer, model):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "  with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "\n",
        "  hidden_states = output.last_hidden_state\n",
        "\n",
        "  mask = inputs['attention_mask'].unsqueeze(-1).float()\n",
        "\n",
        "  sum_embeddings = torch.sum(hidden_states * mask, dim=1)\n",
        "\n",
        "  sum_mask = torch.max(torch.sum(mask, dim=1), torch.tensor(1e-9))\n",
        "\n",
        "  mean_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "  return mean_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SieO7WGD1ii8",
      "metadata": {
        "id": "SieO7WGD1ii8"
      },
      "source": [
        "**Applying DeBERTa Embedding to the companies and labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZeSfImD1pRp",
      "metadata": {
        "id": "HZeSfImD1pRp"
      },
      "outputs": [],
      "source": [
        "rows = insurance_df.shape[0]\n",
        "company_text = []\n",
        "\n",
        "for i in range(rows):\n",
        "    string_row = create_text_about_company(insurance_df.iloc[i])\n",
        "    company_text.append(string_row)\n",
        "\n",
        "company_embeddings_list = [deberta_embeddings(text, tokenizer, model) for text in company_text]\n",
        "company_embeddings = torch.stack(company_embeddings_list).squeeze()\n",
        "\n",
        "label_embeddings_list = [deberta_embeddings(label, tokenizer, model) for label in label_df['label'].tolist()]\n",
        "label_embeddings = torch.stack(label_embeddings_list).squeeze()\n",
        "\n",
        "similarity = util.cos_sim(company_embeddings, label_embeddings)\n",
        "\n",
        "print(company_embeddings.shape)\n",
        "print(label_embeddings.shape)\n",
        "print(similarity.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigning the best label to the company**"
      ],
      "metadata": {
        "id": "Lpd2pypy92Np"
      },
      "id": "Lpd2pypy92Np"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(rows):\n",
        "  company_similarity = similarity[i]\n",
        "\n",
        "  top_labels = company_similarity.topk(k=5)\n",
        "\n",
        "  label_index = top_labels.indices[0].item()\n",
        "  label_name = label_df.iloc[label_index]['label']\n",
        "\n",
        "  insurance_df.loc[i, \"label_insurance\"] = label_name\n",
        "\n",
        "insurance_df.to_csv(\"ml_insurance_challenge_with_deberta.csv\")"
      ],
      "metadata": {
        "id": "6DagbK9V98nM"
      },
      "id": "6DagbK9V98nM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_csv = pd.read_csv(\"ml_insurance_challenge_with_deberta.csv\")\n",
        "deberta_csv['label_insurance'].value_counts()"
      ],
      "metadata": {
        "id": "7pvxo5pPKt9i"
      },
      "id": "7pvxo5pPKt9i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Despite its good recognition as a pretrained model, DeBERTav3 failed to assign the labels correctly, assigning the label \"Non-Alcoholic Beverage Manufacturing\" to 8815 companies. This might be due to the embeddings as they are manually calculated. Therefore, I decided to stick to the results obtained before who have less mistakes.**"
      ],
      "metadata": {
        "id": "NePVbrmqgM7H"
      },
      "id": "NePVbrmqgM7H"
    },
    {
      "cell_type": "code",
      "source": [
        "bi_csv = pd.read_csv(\"ml_insurance_challenge_with_bi_encoder.csv\")\n",
        "bi_plus_cross_csv = pd.read_csv(\"ml_insurance_challenge_with_bi_and_cross_encoder.csv\")"
      ],
      "metadata": {
        "id": "o0jl8KhGVzKS"
      },
      "id": "o0jl8KhGVzKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_csv['label_insurance'].value_counts()"
      ],
      "metadata": {
        "id": "mrdpmac6XaPy"
      },
      "id": "mrdpmac6XaPy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_plus_cross_csv['label_insurance'].value_counts()"
      ],
      "metadata": {
        "id": "hHvMBbw-XgEs"
      },
      "id": "hHvMBbw-XgEs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(classification_report(bi_csv['label_insurance'], bi_plus_cross_csv['label_insurance']))"
      ],
      "metadata": {
        "id": "TtgWn09OYUPb"
      },
      "id": "TtgWn09OYUPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observing the results from the Bi encoding and the Bi and Cross encoding, we notice that the Bi and Cross encoding tends to assign the label \"Non-Alcoholic Beverage Manufacturing\" more than the Bi encoding, therefore being less precise. Therefore, the best result is obtained with a Bi encoder.**"
      ],
      "metadata": {
        "id": "SC97JYwBY1a1"
      },
      "id": "SC97JYwBY1a1"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}